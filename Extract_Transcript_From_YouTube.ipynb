{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting text data from youtube videos\n",
    "\n",
    "### Overview\n",
    "\n",
    "This workbook provides a short illustration of how to download the transcripts for a series of youtube videos, store them in pandas dataframes, and run a few queries to show the kind of analysis you may be interested in doing in your own research. \n",
    "\n",
    "### Software\n",
    "\n",
    "This workbook uses the following modules.\n",
    "\n",
    "* Pandas\n",
    "* PandaSQL\n",
    "* youtube_transcript_api\n",
    "\n",
    "### Background\n",
    "\n",
    "In my consulting sessions with researchers, I've been getting more questions lately about how to extract text with time stamps from videos and images. This makes a certain amount of sense, with the amount of public data now posted online. \n",
    "\n",
    "Youtube often provides a text transcript (captions) along with time stamps. You can access this transcript without any programming through the youtube website. Here are instructions:\n",
    "\n",
    "https://ccm.net/faq/40644-how-to-get-the-transcript-of-a-youtube-video\n",
    "\n",
    "If you have a large number of videos, you may want to avoid a lot of manual downloading and formatting and use a python script. Fortunately, an open source module, \"youtube_transcipt_api\", provides an easy API for this task.\n",
    "\n",
    "https://pypi.org/project/youtube-transcript-api/\n",
    "\n",
    "\n",
    "#### Note - Getting text data from my own videos\n",
    "\n",
    "If you have videos and don't mind making them public or unlisted, you can use this approach by uploading them to youtube and using the methods here. If you have a very large dataset, you might want to use a cloud storage and API solution. Various platforms provide this - here's a link to the google API.\n",
    "\n",
    "https://cloud.google.com/video-intelligence/docs/text-detection\n",
    "\n",
    "This takes a little more programming and configuration, and may result in some cloud computing charges depending depending on the amount of data you want to process, but it is probably more scalable and can offer a more secure environment for videos you want to keep private.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "\n",
    "This workbook reads the text from a series of youtube videos,\n",
    "formats them in a python dataframe, and queries them by timestamp and text strings.\n",
    "\n",
    "For illustration, we'll use a series of lectures from \"On Power and Politics in Today's World\"\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLh9mgdi4rNeyViG2ar68jkgEi4y6doNZy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import the youtube_transcript_api\n",
    "\n",
    "You'll need to install the module before you can import it. You only have to do this once on your system (even if you use it in a different notebook or python script), so you may want to comment out or remove this line after running it once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the transcript from one video\n",
    "\n",
    "We'll extract the transcript for each video using the YouTubeTransciptAPI get_transcript() method. This method takes the video ID as a parameter.\n",
    "\n",
    "You can get the video ID from the URL on youtube - for example, https://www.youtube.com/watch?v=BDqvzFY72mg has the ID 'BDqvzFY72mg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = YouTubeTranscriptApi.get_transcript('BDqvzFY72mg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method returns the transcript of the video as a list of lines, each stored as a dictionary. We get 1280 lines from the video above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(transcript))\n",
    "print(len(transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few lines. Each line contains a dictionary with keys \"text\", \"start\", and \"duratation\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can parse the video using standard techiques for JSON or dictionaries (more info here: https://github.com/geoffswc/Python-JSON-Workshop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcripts for Multiple Videos in Pandas Format\n",
    "\n",
    "Fortunately, this is a flat dictionary structure, not deeply nested, so we can convert this to a pandas dataframe easily. In this next section, we'll review code to convert a series of videos and concatenate them into a single data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll greate a list of IDs for each video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\n",
    "    'BDqvzFY72mg',\n",
    "    'f5nbT4xQqwI',\n",
    "    's48b9B5gd88',\n",
    "    '4eUS8trd_yI',\n",
    "    'aKW_Vsk4hzs',\n",
    "    'q53DF6ySOZg',\n",
    "    'T3-VlQu3iRM'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the transcript for each video using the YouTubeTransciptAPI get_transcript() method. \n",
    "\n",
    "For now, we will store the transcript for each video in a list named transcripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = []\n",
    "for v in links:\n",
    "    try:\n",
    "        df = pd.DataFrame(YouTubeTranscriptApi.get_transcript(v))\n",
    "        df['video_id'] = v\n",
    "        transcripts.append(df)\n",
    "    except:\n",
    "        print(v, 'failed to translate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have now created a list of pandas dataframes. Let's take a look at a few lines from the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll combine all the dataframes into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcripts = pd.concat(transcripts).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_transcripts.iloc[1000:1100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run some queries\n",
    "\n",
    "Now that we have our text in a single dataframe, we can analyze it using a wide range of tools in python. You might be interested in natural language processing, sentiment analysis, text classification, lexical structures, regional differences in language ussed in school board meeings... more than we can get into here (though feel free to get started here with the Library \"Document Classification with Scikit-Learn\" workshop aat https://courses.ucsf.edu/course/view.php?id=8249)\n",
    "\n",
    "For now, we'll just query the data in a few ways and leave it there. If you've taken any of my workshops, you'll know I lean toward using SQL, so I'll write a very queries using the pandasql module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip isntall pandasql \n",
    "from pandasql import sqldf \n",
    "pysqldf = lambda q: sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which videos have the most lines of text\n",
    "pysqldf(\"SELECT video_id, COUNT(*) FROM df_transcripts GROUP BY video_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which videos were longest (highest timestamp + duration)\n",
    "pysqldf(\"SELECT video_id, MAX(start + duration) FROM df_transcripts GROUP BY video_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most mentions of the Cold War\n",
    "pysqldf(\"\"\"\n",
    "SELECT \n",
    "    video_id, \n",
    "    COUNT(1) \n",
    "FROM \n",
    "    df_transcripts \n",
    "WHERE \n",
    "    LOWER(text) LIKE ('%cold war%')\n",
    "GROUP BY \n",
    "    video_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what rows matched\n",
    "pysqldf(\"\"\"\n",
    "SELECT \n",
    "    *\n",
    "FROM \n",
    "    df_transcripts \n",
    "WHERE \n",
    "    LOWER(text) LIKE ('%cold war%')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when was cold war first mentioned in each video?\n",
    "pysqldf(\"\"\"\n",
    "SELECT \n",
    "    video_id, \n",
    "    MIN(start) \n",
    "FROM \n",
    "    df_transcripts \n",
    "WHERE \n",
    "    LOWER(text) LIKE ('%cold war%')\n",
    "GROUP BY \n",
    "    video_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aKW_Vsk4hzs\n",
    "transcript = pd.DataFrame(YouTubeTranscriptApi.get_transcript('q53DF6ySOZg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(transcript['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcripts = df_transcripts.replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcripts.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
